{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this Notebook:\n",
    "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
    "\n",
    "When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. \n",
    "We are going to look at the following for each channel of communications:\n",
    "\n",
    "Most common words - find these and create word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text # Contains the stop word list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHATS</th>\n",
       "      <th>EMAILS</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abn</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerated</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerating</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accenture</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CHATS  EMAILS  SMS\n",
       "able              1       3    0\n",
       "abn               0       2    0\n",
       "accelerated       0       1    0\n",
       "accelerating      0       1    0\n",
       "accenture         0       5    0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.read_pickle('term_matrix.pickle')\n",
    "#Transpose becuse it's harder to operate across rows. Easier across columns.\n",
    "#We want to aggregate for each comedian. So comedians should be on the columns.\n",
    "data = data_clean.transpose() \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CHATS': [('hi', 15),\n",
       "  ('im', 13),\n",
       "  ('long', 11),\n",
       "  ('going', 10),\n",
       "  ('john', 10),\n",
       "  ('short', 9),\n",
       "  ('time', 9),\n",
       "  ('just', 8),\n",
       "  ('good', 7),\n",
       "  ('trading', 7),\n",
       "  ('like', 6),\n",
       "  ('risk', 6),\n",
       "  ('chart', 6),\n",
       "  ('trade', 6),\n",
       "  ('yeah', 6),\n",
       "  ('guys', 5),\n",
       "  ('dont', 5),\n",
       "  ('phil', 5),\n",
       "  ('hello', 4),\n",
       "  ('today', 4),\n",
       "  ('eurgbp', 4),\n",
       "  ('look', 4),\n",
       "  ('moment', 4),\n",
       "  ('looking', 4),\n",
       "  ('right', 4),\n",
       "  ('later', 4),\n",
       "  ('steve', 3),\n",
       "  ('downtrend', 3),\n",
       "  ('hows', 3),\n",
       "  ('yes', 3)],\n",
       " 'EMAILS': [('email', 92),\n",
       "  ('buy', 80),\n",
       "  ('information', 54),\n",
       "  ('phillip', 50),\n",
       "  ('message', 44),\n",
       "  ('click', 32),\n",
       "  ('downgraded', 32),\n",
       "  ('use', 31),\n",
       "  ('account', 31),\n",
       "  ('subject', 28),\n",
       "  ('thanks', 28),\n",
       "  ('need', 28),\n",
       "  ('know', 27),\n",
       "  ('original', 27),\n",
       "  ('new', 27),\n",
       "  ('request', 27),\n",
       "  ('enron', 27),\n",
       "  ('password', 26),\n",
       "  ('strong', 25),\n",
       "  ('time', 25),\n",
       "  ('review', 25),\n",
       "  ('recipient', 24),\n",
       "  ('like', 23),\n",
       "  ('sent', 23),\n",
       "  ('change', 22),\n",
       "  ('let', 22),\n",
       "  ('receive', 22),\n",
       "  ('price', 21),\n",
       "  ('thank', 21),\n",
       "  ('stock', 20)],\n",
       " 'SMS': [('este', 2),\n",
       "  ('鞋子全部打湿完了', 1),\n",
       "  ('getting', 1),\n",
       "  ('exemplo', 1),\n",
       "  ('hola', 1),\n",
       "  ('della', 1),\n",
       "  ('um', 1),\n",
       "  ('critical', 1),\n",
       "  ('polskiego', 1),\n",
       "  ('maltija', 1),\n",
       "  ('say', 1),\n",
       "  ('olá', 1),\n",
       "  ('ένα', 1),\n",
       "  ('αυτό', 1),\n",
       "  ('γεια', 1),\n",
       "  ('γλώσσας', 1),\n",
       "  ('είναι', 1),\n",
       "  ('ελληνικής', 1),\n",
       "  ('open', 1),\n",
       "  ('języka', 1),\n",
       "  ('σας', 1),\n",
       "  ('questo', 1),\n",
       "  ('thx', 1),\n",
       "  ('italiana', 1),\n",
       "  ('ejemplo', 1),\n",
       "  ('iteresting', 1),\n",
       "  ('portuguesa', 1),\n",
       "  ('língua', 1),\n",
       "  ('saluti', 1),\n",
       "  ('hi', 1)]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 30 words said by each comedian\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    top = data[c].sort_values(ascending=False).head(30)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATS\n",
      "hi, im, long, going, john, short, time, just, good, trading, like, risk, chart, trade\n",
      "---\n",
      "EMAILS\n",
      "email, buy, information, phillip, message, click, downgraded, use, account, subject, thanks, need, know, original\n",
      "---\n",
      "SMS\n",
      "este, 鞋子全部打湿完了, getting, exemplo, hola, della, um, critical, polskiego, maltija, say, olá, ένα, αυτό\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Print the top 15 words said by each channel\n",
    "for channel, top_words in top_dict.items():\n",
    "    print(channel)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'im',\n",
       " 'long',\n",
       " 'going',\n",
       " 'john',\n",
       " 'short',\n",
       " 'time',\n",
       " 'just',\n",
       " 'good',\n",
       " 'trading',\n",
       " 'like',\n",
       " 'risk',\n",
       " 'chart',\n",
       " 'trade',\n",
       " 'yeah',\n",
       " 'guys',\n",
       " 'dont',\n",
       " 'phil',\n",
       " 'hello',\n",
       " 'today',\n",
       " 'eurgbp',\n",
       " 'look',\n",
       " 'moment',\n",
       " 'looking',\n",
       " 'right',\n",
       " 'later',\n",
       " 'steve',\n",
       " 'downtrend',\n",
       " 'hows',\n",
       " 'yes',\n",
       " 'email',\n",
       " 'buy',\n",
       " 'information',\n",
       " 'phillip',\n",
       " 'message',\n",
       " 'click',\n",
       " 'downgraded',\n",
       " 'use',\n",
       " 'account',\n",
       " 'subject',\n",
       " 'thanks',\n",
       " 'need',\n",
       " 'know',\n",
       " 'original',\n",
       " 'new',\n",
       " 'request',\n",
       " 'enron',\n",
       " 'password',\n",
       " 'strong',\n",
       " 'time',\n",
       " 'review',\n",
       " 'recipient',\n",
       " 'like',\n",
       " 'sent',\n",
       " 'change',\n",
       " 'let',\n",
       " 'receive',\n",
       " 'price',\n",
       " 'thank',\n",
       " 'stock',\n",
       " 'este',\n",
       " '鞋子全部打湿完了',\n",
       " 'getting',\n",
       " 'exemplo',\n",
       " 'hola',\n",
       " 'della',\n",
       " 'um',\n",
       " 'critical',\n",
       " 'polskiego',\n",
       " 'maltija',\n",
       " 'say',\n",
       " 'olá',\n",
       " 'ένα',\n",
       " 'αυτό',\n",
       " 'γεια',\n",
       " 'γλώσσας',\n",
       " 'είναι',\n",
       " 'ελληνικής',\n",
       " 'open',\n",
       " 'języka',\n",
       " 'σας',\n",
       " 'questo',\n",
       " 'thx',\n",
       " 'italiana',\n",
       " 'ejemplo',\n",
       " 'iteresting',\n",
       " 'portuguesa',\n",
       " 'língua',\n",
       " 'saluti',\n",
       " 'hi']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the most common top words --> add them to the stop word list\n",
    "\n",
    "# Let's first create a list that just has each channel top 30 words (even if repeated)\n",
    "words = []\n",
    "for chanel in data.columns:\n",
    "    top = [word for (word, count) in top_dict[chanel]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "        \n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 2),\n",
       " ('time', 2),\n",
       " ('like', 2),\n",
       " ('im', 1),\n",
       " ('long', 1),\n",
       " ('going', 1),\n",
       " ('john', 1),\n",
       " ('short', 1),\n",
       " ('just', 1),\n",
       " ('good', 1),\n",
       " ('trading', 1),\n",
       " ('risk', 1),\n",
       " ('chart', 1),\n",
       " ('trade', 1),\n",
       " ('yeah', 1),\n",
       " ('guys', 1),\n",
       " ('dont', 1),\n",
       " ('phil', 1),\n",
       " ('hello', 1),\n",
       " ('today', 1),\n",
       " ('eurgbp', 1),\n",
       " ('look', 1),\n",
       " ('moment', 1),\n",
       " ('looking', 1),\n",
       " ('right', 1),\n",
       " ('later', 1),\n",
       " ('steve', 1),\n",
       " ('downtrend', 1),\n",
       " ('hows', 1),\n",
       " ('yes', 1),\n",
       " ('email', 1),\n",
       " ('buy', 1),\n",
       " ('information', 1),\n",
       " ('phillip', 1),\n",
       " ('message', 1),\n",
       " ('click', 1),\n",
       " ('downgraded', 1),\n",
       " ('use', 1),\n",
       " ('account', 1),\n",
       " ('subject', 1),\n",
       " ('thanks', 1),\n",
       " ('need', 1),\n",
       " ('know', 1),\n",
       " ('original', 1),\n",
       " ('new', 1),\n",
       " ('request', 1),\n",
       " ('enron', 1),\n",
       " ('password', 1),\n",
       " ('strong', 1),\n",
       " ('review', 1),\n",
       " ('recipient', 1),\n",
       " ('sent', 1),\n",
       " ('change', 1),\n",
       " ('let', 1),\n",
       " ('receive', 1),\n",
       " ('price', 1),\n",
       " ('thank', 1),\n",
       " ('stock', 1),\n",
       " ('este', 1),\n",
       " ('鞋子全部打湿完了', 1),\n",
       " ('getting', 1),\n",
       " ('exemplo', 1),\n",
       " ('hola', 1),\n",
       " ('della', 1),\n",
       " ('um', 1),\n",
       " ('critical', 1),\n",
       " ('polskiego', 1),\n",
       " ('maltija', 1),\n",
       " ('say', 1),\n",
       " ('olá', 1),\n",
       " ('ένα', 1),\n",
       " ('αυτό', 1),\n",
       " ('γεια', 1),\n",
       " ('γλώσσας', 1),\n",
       " ('είναι', 1),\n",
       " ('ελληνικής', 1),\n",
       " ('open', 1),\n",
       " ('języka', 1),\n",
       " ('σας', 1),\n",
       " ('questo', 1),\n",
       " ('thx', 1),\n",
       " ('italiana', 1),\n",
       " ('ejemplo', 1),\n",
       " ('iteresting', 1),\n",
       " ('portuguesa', 1),\n",
       " ('língua', 1),\n",
       " ('saluti', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate this list and identify the most common words along with how many comedian's routines they occur in\n",
    "Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If more than half of the channel have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 5]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-96d4c5f4e87c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create subplots for each channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchan\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mr\"\\w[\\w']+\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;31m# remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Reset the output dimensions\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "channels = ['SMS', 'EMAILS', 'CHATS']\n",
    "\n",
    "# Create subplots for each channel\n",
    "for index, chan in enumerate(data.columns):\n",
    "    wc.generate(data[chan])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_names[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
